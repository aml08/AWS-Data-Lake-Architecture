# â˜ï¸ AWS Data Lake Architecture - ShopNow Project

![AWS](https://img.shields.io/badge/AWS-Console-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Athena](https://img.shields.io/badge/Amazon_Athena-SQL-blue?style=for-the-badge)
![Glue](https://img.shields.io/badge/AWS_Glue-ETL-blue?style=for-the-badge)
![IAM](https://img.shields.io/badge/Security-IAM_Governance-red?style=for-the-badge)

## ğŸ“‹ PrÃ©sentation du Projet
Conception et dÃ©ploiement d'une architecture **Data Lake complÃ¨te sur AWS** pour une entreprise E-commerce.
L'objectif Ã©tait de centraliser les donnÃ©es transactionnelles (PostgreSQL) et de les prÃ©parer pour l'analyse, tout en respectant le principe de **moindre privilÃ¨ge**.

> **MÃ©thodologie :** DÃ©ploiement via la Console AWS (Infrastructure as a Service).

---

## ğŸ—ï¸ Architecture DÃ©ployÃ©e

![Architecture Diagram](architecture_diagram.png)

Le pipeline de donnÃ©es suit les Ã©tapes suivantes :
1.  **Source :** Base de donnÃ©es transactionnelle **Amazon RDS (PostgreSQL)** contenant les tables `clients`, `produits`, `ventes`.
2.  **Stockage (Data Lake) :**
    * `s3://shopnow-data/raw/` : Stockage des extractions brutes (CSV).
    * `s3://shopnow-data/analytics/` : DonnÃ©es nettoyÃ©es et optimisÃ©es via Glue.
3.  **ETL & Catalogue :** Utilisation d'**AWS Glue** (Crawlers & Jobs) pour cataloguer les donnÃ©es et transformer les types (casting, nettoyage).
4.  **Analyse :** Validation et exploration des donnÃ©es via **Amazon Athena** (SQL Serverless).

### ğŸ” Zoom sur le Pipeline ETL (AWS Glue Studio)
J'ai conÃ§u un workflow visuel pour joindre les tables `Ventes`, `Clients` et `Produits` et nettoyer les donnÃ©es avant l'export S3 :

![Glue Job Workflow](glue job.png)

---

## ğŸ›¡ï¸ SÃ©curitÃ© & Gouvernance (IAM)

Point central du projet : la mise en place d'une politique de sÃ©curitÃ© stricte (RBAC - Role Based Access Control).
J'ai configurÃ© **3 rÃ´les IAM distincts** pour cloisonner les accÃ¨s :

### 1. RÃ´le `Data Engineer`
* **ResponsabilitÃ© :** Maintenance du pipeline et transformation.
* **Permissions :**
    * âœ… Lecture/Ã‰criture sur tous les Buckets S3 (`raw` et `analytics`).
    * âœ… AccÃ¨s complet Ã  AWS Glue (CrÃ©ation de Jobs/Crawlers).
    * âœ… AccÃ¨s Ã  Athena.
    * âš ï¸ Lecture seule sur RDS (Protection de la prod).

### 2. RÃ´le `Data Analyst`
* **ResponsabilitÃ© :** CrÃ©ation de rapports et requÃªtes SQL.
* **Permissions :**
    * âœ… AccÃ¨s Ã  Amazon Athena pour requÃªter le Data Lake.
    * âš ï¸ Lecture seule stricte sur S3 (`analytics/` uniquement).
    * âŒ **Interdiction** d'accÃ¨s Ã  la zone `raw/` ou aux jobs Glue.

---

## ğŸ“Š Validation Technique & RÃ©sultats

Le pipeline ETL est fonctionnel et les donnÃ©es sont prÃªtes Ã  Ãªtre consommÃ©es.
Validation effectuÃ©e via des requÃªtes SQL complexes dans **Amazon Athena** :

![Athena SQL Results](athena_sql_results.png)

*Exemple de requÃªte analytique validÃ©e (Chiffre d'Affaires par CatÃ©gorie) :*

```sql
SELECT 
    p.categorie, 
    SUM(v.montant) as chiffre_affaires_total
FROM "shopnow_db"."ventes" v
JOIN "shopnow_db"."produits" p ON v.id_produit = p.id_produit
GROUP BY p.categorie
ORDER BY chiffre_affaires_total DESC;
```
## ğŸ› ï¸ Services AWS MaÃ®trisÃ©s
* Amazon S3 : Partitionnement et cycle de vie des donnÃ©es (Raw vs Analytics).
* AWS Glue : Data Catalog, Crawlers et ETL Jobs (Visual).
* Amazon Athena : CrÃ©ation de tables externes et requÃªtes SQL d'analyse.
* IAM : CrÃ©ation de StratÃ©gies et RÃ´les.

